{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Reweight Examples for Robust Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielTan\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from model import *\n",
    "from data_loader import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'lr' : 1e-3,\n",
    "    'momentum' : 0.9,\n",
    "    'batch_size' : 100,\n",
    "    'num_iterations' : 8000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Following the class imbalance experiment in the paper, we used numbers 9 and 4 of the MNIST dataset to form a highly imbalanced dataset where 9 is the dominating class. The test set on the other hand is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_mnist_loader(hyperparameters['batch_size'], classes=[9, 4], proportion=0.995, mode=\"train\")\n",
    "test_loader = get_mnist_loader(hyperparameters['batch_size'], classes=[9, 4], proportion=0.5, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, requires_grad=True):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the validation data is small (only 10 examples) there is no need to wrap it in a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = to_var(data_loader.dataset.data_val, requires_grad=False)\n",
    "val_labels = to_var(data_loader.dataset.labels_val, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 32, 32]) tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "for i,(img, label) in enumerate(data_loader):\n",
    "    print(img.size(),label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 32, 32]) tensor([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,\n",
      "         1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,\n",
      "         1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,\n",
      "         0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
      "         0.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
      "         0.,  1.,  0.,  1.])\n"
     ]
    }
   ],
   "source": [
    "for i,(img, label) in enumerate(test_loader):\n",
    "    print(img.size(),label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5a63b67606df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\NTUST\\Codes\\LearnToReweight\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_out)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMetaConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\NTUST\\Codes\\LearnToReweight\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_var' is not defined"
     ]
    }
   ],
   "source": [
    "net = LeNet(n_out=1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "    torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(net.params(),lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "I trained a LeNet model for the MNIST data without weighting the loss as a baseline model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_losses = []\n",
    "plot_step = 100\n",
    "\n",
    "smoothing_alpha = 0.9\n",
    "accuracy_log = []\n",
    "for i in tqdm(range(hyperparameters['num_iterations'])):\n",
    "    net.train()\n",
    "    image, labels = next(iter(data_loader))\n",
    "\n",
    "    image = to_var(image, requires_grad=False)\n",
    "    labels = to_var(labels, requires_grad=False)\n",
    "    \n",
    "    # real network\n",
    "    y = net(image)\n",
    "    cost = F.binary_cross_entropy_with_logits(y, labels)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    net_l = smoothing_alpha *net_l + (1 - smoothing_alpha)* cost.item()\n",
    "    net_losses.append(net_l/(1 - smoothing_alpha**(i+1)))\n",
    "    \n",
    "    if i % plot_step == 0:\n",
    "        net.eval()\n",
    "        \n",
    "        acc = []\n",
    "        for itr,(test_img, test_label) in enumerate(test_loader):\n",
    "            test_img = to_var(test_img, requires_grad=False)\n",
    "            test_label = to_var(test_label, requires_grad=False)\n",
    "            \n",
    "            output = net(test_img)\n",
    "            predicted = (F.sigmoid(output) > 0.5).int()\n",
    "            \n",
    "            acc.append((predicted.int() == test_label.int()).float())\n",
    "\n",
    "        accuracy = torch.cat(acc,dim=0).mean()\n",
    "        accuracy_log.append(np.array([i,accuracy])[None])\n",
    "        \n",
    "        \n",
    "        IPython.display.clear_output()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(13,5))\n",
    "        ax1, ax2 = axes.ravel()\n",
    "\n",
    "        ax1.plot(net_losses, label='net_losses')\n",
    "        ax1.set_ylabel(\"Losses\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        acc_log = np.concatenate(accuracy_log, axis=0)\n",
    "        ax2.plot(acc_log[:,0],acc_log[:,1])\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, due to the heavily imbalanced training data, the network could not learn how to differentiate between 9 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Reweight Examples \n",
    "Below is a pseudocode of the method proposed in the paper. It is very straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pseudocode](pseudocode.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_losses_clean = []\n",
    "net_losses = []\n",
    "plot_step = 100\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "smoothing_alpha = 0.9\n",
    "meta_l = 0\n",
    "net_l = 0\n",
    "accuracy_log = []\n",
    "for i in tqdm(range(hyperparameters['num_iterations'])):\n",
    "    net.train()\n",
    "    image, labels = next(iter(data_loader))\n",
    "    # since validation data is small I just fixed them instead of building an iterator\n",
    "    # initialize a dummy network for the meta learning of the weights\n",
    "    meta_net = LeNet(n_out=1)\n",
    "    meta_net.load_state_dict(net.state_dict())\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        meta_net.cuda()\n",
    "    \n",
    "    image = to_var(image, requires_grad=False)\n",
    "    labels = to_var(labels, requires_grad=False)\n",
    "    \n",
    "    y_f_hat  = meta_net(image)\n",
    "    cost = F.binary_cross_entropy_with_logits(y_f_hat,labels, reduce=False)\n",
    "    eps = to_var(torch.zeros(cost.size()))\n",
    "    l_f_meta = torch.sum(cost * eps)\n",
    "\n",
    "    meta_net.zero_grad()\n",
    "\n",
    "    grads = torch.autograd.grad(l_f_meta, (meta_net.params()), create_graph=True)\n",
    "    \n",
    "    meta_net.update_params(hyperparameters['lr'], source_params=grads)\n",
    "    \n",
    "    y_g_hat = meta_net(val_data)\n",
    "\n",
    "    l_g_meta = F.binary_cross_entropy_with_logits(y_g_hat,val_labels)\n",
    "\n",
    "    grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]\n",
    "\n",
    "    w_tilde = torch.clamp(-grad_eps,min=0)\n",
    "    norm_c = torch.sum(w_tilde)\n",
    "\n",
    "    if norm_c != 0:\n",
    "        w = w_tilde / norm_c\n",
    "    else:\n",
    "        w = w_tilde\n",
    "\n",
    "    # real network\n",
    "    y_f_hat = net(image)\n",
    "    cost = F.binary_cross_entropy_with_logits(y_f_hat, labels, reduce=False)\n",
    "    l_f = torch.sum(cost * w)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    l_f.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    meta_l = smoothing_alpha *meta_l + (1 - smoothing_alpha)* l_g_meta.item()\n",
    "    meta_losses_clean.append(meta_l/(1 - smoothing_alpha**(i+1)))\n",
    "    \n",
    "    net_l = smoothing_alpha *net_l + (1 - smoothing_alpha)* l_f.item()\n",
    "    net_losses.append(net_l/(1 - smoothing_alpha**(i+1)))\n",
    "    \n",
    "    if i % plot_step == 0:\n",
    "        net.eval()\n",
    "        \n",
    "        acc = []\n",
    "        for itr,(test_img, test_label) in enumerate(test_loader):\n",
    "            test_img = to_var(test_img, requires_grad=False)\n",
    "            test_label = to_var(test_label, requires_grad=False)\n",
    "            \n",
    "            output = net(test_img)\n",
    "            predicted = (F.sigmoid(output) > 0.5).int()\n",
    "            \n",
    "            acc.append((predicted.int() == test_label.int()).float())\n",
    "\n",
    "        accuracy = torch.cat(acc,dim=0).mean()\n",
    "        accuracy_log.append(np.array([i,accuracy])[None])\n",
    "        \n",
    "        \n",
    "        IPython.display.clear_output()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(13,5))\n",
    "        ax1, ax2 = axes.ravel()\n",
    "\n",
    "        ax1.plot(meta_losses_clean, label='meta_losses_clean')\n",
    "        ax1.plot(net_losses, label='net_losses')\n",
    "        ax1.set_ylabel(\"Losses\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        acc_log = np.concatenate(accuracy_log, axis=0)\n",
    "        ax2.plot(acc_log[:,0],acc_log[:,1])\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        plt.show()\n",
    "        \n",
    "#         print(accuracy)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
